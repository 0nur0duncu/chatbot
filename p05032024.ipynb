{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer\n",
    "from langchain.chains import RetrievalQA\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import set_seed\n",
    "from IPython.display import clear_output, display, Markdown\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 128\n",
    "directory = \"./pdf\"\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2doc(directory:str) -> list:\n",
    "  return DirectoryLoader(directory, glob=\"*.pdf\", loader_cls=PyPDFLoader).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(docs:list) -> list:\n",
    "  return RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['GOOGLE_API_KEY'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "model_kwargs = {'device':'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding=GPT4AllEmbeddings(model_kwargs={\"device\": \"cuda\"})\n",
    "def vector_storing(splitted_text):\n",
    "  return Chroma.from_documents(documents=splitted_text,\n",
    "                               embedding=GPT4AllEmbeddings(model_kwargs={\"device\": \"cuda\"}),\n",
    "                               persist_directory='db'\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = vector_storing(split_text(file2doc(directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vectorstore(prompt,vectorstore):\n",
    "  return vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6}).get_relevant_documents(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vectorstore(\"Güneş nedir?\",vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512, cache_dir=\"./model\",)\n",
    "question_answerer = pipeline(\n",
    "    \"question-answering\", \n",
    "    model=model_name, \n",
    "    tokenizer=tokenizer,\n",
    "    cache_dir=\"./model\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=question_answerer,\n",
    "    model_kwargs={\"temperature\": 0.3, \"max_length\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  cache_dir=\"./model\",\n",
    "  device_map=\"auto\",\n",
    "  quantization_config=quantization_config\n",
    "  )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", cache_dir=\"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  cache_dir=\"./model\",\n",
    "  device_map=\"auto\"\n",
    "  )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", cache_dir=\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "llm=HuggingFaceHub(\n",
    "  repo_id=model_name,\n",
    "  model_kwargs={\"temperature\":0.7},\n",
    "  huggingfacehub_api_token=os.environ[\"HUGGIGFACEHUB_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def chat_with_llama(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to('cuda')\n",
    "    output = llm.generate(input_ids, max_length=256, num_beams=4, no_repeat_ngram_size=2)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "while True:\n",
    "    prompt = input(\"You: \")\n",
    "    response = chat_with_llama(prompt)\n",
    "    print(\"Llama:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text = 'Hamburg is in which country?\\n'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated_ids = llm.generate(input_ids, max_length=MAX_NEW_TOKENS)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "  Soruyu verilen bağlama göre en anlaşılır ve detaylı şekilde cevapla.\n",
    "  Gelen sorular karşılaştırma sorusu, genel sorular veya direk bilgi istenen sorular olabilir.\n",
    "  Karşılaştırma sorularına bağlamdan anlamlı bir sonuç çıkararak cevap vereceksin.\n",
    "  Soruları yanıtlarken sadece Türklerin bakış açısından cevapla.\n",
    "  Sana sağlanan dokümanlarda bilgisi bulunmayan bir bağlama yanıt olarak \"Metinde bilgi bulunmamaktadır\" veya \"Bilmiyorum\" cevabını vereceksin.\n",
    "\n",
    "  Context:\\n {context}?\\n\n",
    "  Question: \\n{question}\\n\n",
    "  \n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "\"\"\"import os\n",
    "os.environ['GOOGLE_API_KEY'] =  GOOGLE_API_KEY\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\"\"\"\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•','*')\n",
    "  return Markdown(textwrap.indent(text, '>', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(to_markdown(llm.invoke(\"Güneş nedir?\").content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "    print(qa_chain({\"query\": question})['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "  question = input(\"Please ask a question: \")\n",
    "  if question == '':\n",
    "    break\n",
    "  else:\n",
    "    clear_output(wait=True)\n",
    "    Markdown(ask(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
