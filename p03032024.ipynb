{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- https://medium.com/@sevketay09/gemini-pro-ile-multimodal-rag-langchain-chroma-868a379c85b8<br>\n",
    "-- https://www.linkedin.com/pulse/extract-text-from-any-pdf-image-dataset-generation-jagrat-patel/<br>\n",
    "-- https://medium.com/@princekrampah/prompt-templates-in-langchain-248c015be3e0<br>\n",
    "-- https://medium.com/@onkarmishra/using-langchain-for-question-answering-on-own-data-3af0a82789ed<br>\n",
    "-- https://medium.com/@epappas/dealing-with-vector-dimension-mismatch-my-experience-with-openai-mbeddings-and-qdrant-vector-20a6e13b6d9f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import textwrap\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "from langchain_community.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "file_path = \"./sample_01.pdf\"\n",
    "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('•','*')\n",
    "  return Markdown(textwrap.indent(text, '>', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2text(file_path):\n",
    "    from langchain_community.document_loaders import PyPDFLoader\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pypdf2(file_path):\n",
    "    from PyPDF2 import PdfReader\n",
    "    pdf_reader = PdfReader(file_path)\n",
    "    raw_text = ''\n",
    "    for i, page in enumerate(pdf_reader.pages):\n",
    "      text = page.extract_text()\n",
    "      if text:\n",
    "        raw_text += text\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_text2text(file_path):\n",
    "    from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    # return '\\n'.join(doc.page_content for doc in documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=1000,separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "    return text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(splitted_docs):\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n",
    "    return Chroma.from_documents(documents=splitted_docs, embedding=embeddings,persist_directory='db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_model(prompt):\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "    response = model.generate_content(prompt)\n",
    "    display(to_markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       ">Güneş, gezegenleri ve diğer nesneleri içeren Güneş Sistemimizin merkezindeki dev, sıcak ve parlak bir yıldızdır. Güneş, esas olarak hidrojen ve helyumdan oluşan devasa bir gaz topudur.\n",
       ">\n",
       ">* **Yakıt:** Güneş'in enerjisi, çekirdeğinde gerçekleşen nükleer füzyon reaksiyonlarından gelir. Bu reaksiyonlarda hidrojen helyuma dönüştürülür ve muazzam miktarda enerji açığa çıkar.\n",
       ">* **Sıcaklık:** Güneş'in çekirdeğindeki sıcaklık yaklaşık 27 milyon Kelvin'dir ve yüzey sıcaklığı yaklaşık 5.500 Kelvin'dir.\n",
       ">* **Büyüklük:** Güneş, Dünya'dan yaklaşık 109 kat daha büyük bir çapa sahiptir.\n",
       ">* **Parlaklık:** Güneş, Dünya'dan gelen toplam güneş ışığının yaklaşık %99,9'unu yayar.\n",
       ">* **Yerçekimi:** Güneş, Güneş Sistemindeki tüm nesneleri yörüngelerinde tutan güçlü bir yerçekimine sahiptir.\n",
       ">* **Hayat için hayati önem:** Güneş, Dünya üzerinde yaşam için gerekli olan ışık, ısı ve enerjiyi sağlar."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "default_model(\"Güneş nedir kısaca açıkla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = pdf2text(file_path)\n",
    "vector_index = vectorization(text_split(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "  Soruyu verilen bağlama göre en anlaşılır ve detaylı şekilde cevapla.\n",
    "  Gelen sorular karşılaştırma sorusu, genel sorular veya direk bilgi istenen sorular olabilir.\n",
    "  Karşılaştırma sorularına bağlamdan anlamlı bir sonuç çıkararak cevap vereceksin.\n",
    "  Soruları yanıtlarken sadece Türklerin bakış açısından cevapla.\n",
    "  Sana sağlanan dokümanlarda bilgisi bulunmayan bir bağlama yanıt olarak \"Metinde bilgi bulunmamaktadır\" veya \"Bilmiyorum\" cevabını vereceksin.\n",
    "  Markdown dosyası olarak çıktı ver.\n",
    "\n",
    "  Context:\\n {context}?\\n\n",
    "  Question: \\n{question}\\n\n",
    "\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "# prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"])\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template)\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "# repo_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "# model = HuggingFaceHub(huggingfacehub_api_token=os.environ[\"HUGGIGFACEHUB_API_KEY\"], repo_id=repo_id, model_kwargs={\"temperature\":0.2, \"max_new_tokens\":50})\n",
    "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    model,\n",
    "    retriever=vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6}),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "    print(qa_chain({\"query\": question})['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "  question = input(\"Please ask a question: \")\n",
    "  if question == '':\n",
    "    break\n",
    "  else:\n",
    "    clear_output(wait=True)\n",
    "    ask(question)\n",
    "    #time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
